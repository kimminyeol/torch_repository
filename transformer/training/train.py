import torch
from transformers import AutoModelForSeq2SeqLM
from data.dataloader import get_dataloader
from data.tokenizer import Tokenizer
from utils.seed import set_seed
from utils.checkpoint import save_checkpoint
from training.trainer import Trainer
from training.optimizer import get_optimizer
from training.scheduler import get_scheduler
import sys
import os

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
# ì„¤ì •
set_seed(42)
device = torch.device("mps" if torch.cuda.is_available() else "cpu")

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì¤€ë¹„
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-en-fr").to(device)
tokenizer = Tokenizer(model_name='Helsinki-NLP/opus-mt-en-fr')

# ë°ì´í„° ë¡œë“œ
train_dataloader = get_dataloader("en_fr_data.tsv", tokenizer, batch_size=64)

# ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
optimizer = get_optimizer(model)
scheduler = get_scheduler(optimizer, num_training_steps=len(train_dataloader) * 5)

# Trainer ì´ˆê¸°í™” ë° í•™ìŠµ ì‹œì‘
trainer = Trainer(model, optimizer, scheduler, train_dataloader, device)

print("ğŸ”¥ Training started...")
trainer.train(epochs=2)
print("âœ… Training complete!")


# ì²´í¬í¬ì¸íŠ¸ ì €ì¥
save_checkpoint(model, optimizer, epoch=5)